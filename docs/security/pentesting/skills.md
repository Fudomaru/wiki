---
title: Penetration Test Skills & Mindset
description: Notes on the skills I actually need for penetration testing, written for myself. This is a living curriculum, not a job ad.
---

# Penetration Test Skills & Mindset

This is my place to collect what matters when I do real penetration tests.  
Not "how to get hired" or "check these boxes", 
but the set of capabilities and habits that let me run a useful, safe engagement.

I write this for future-me. 
Short notes, examples I can come back to, 
and a plan for incremental practice.

---

## Why this page exists

Penetration testing sits on top of lots of small competencies. 
Knowing which ones matter lets me prioritize practice and build useful lab exercises. 
This page is the practical bridge between Fundamentals and Structure.

---

## Technical foundations (what I need to be able to do)

- Read a network: identify subnets, services, and trust boundaries from sparse data.  
- Read an OS: find processes, users, permissions, and common misconfigurations on Linux and Windows.  
- Read the web: follow request/response flows, spot auth/session problems, and reason about client-server trust.  
- Read code (enough): skim Python/Javascript/PHP to spot insecure patterns and inputs.  
- Understand auth and crypto basics: what makes a token or cookie weak in practice.

_Practical lab work_  
For each item above, I need a lab exercise: 
capture a pcap and explain it; 
escalate from local shell to root on a test VM; 
break an auth flow in a deliberately insecure app.

---

## Tooling & scripting (how I make things repeatable)

- Know categories more than tools: 
  recon, enumeration, interception, exploitation, post-exploitation, evidence collection.  
- Be fluent in at least one scripting language (Python) and shell scripting (bash). 
  Automate boring stuff and build small proofs of concept quickly.  
- Understand what each tool does under the hood. 
  If a scanner flags something, I should know why it flagged it and how to validate it manually.

_Practical lab work_  
Write a 10-line Python script that automates evidence collection: nmap scan, curl auth test, grab a banner, save artifacts.

---

## Method & mindset (how I think while testing)

- Hypothesis-driven testing: turn a suspicion into a small test, validate, iterate.  
- Chain thinking: always ask how a single finding could be combined into an attack path.  
- Low-noise, high-confidence proofs: prefer reproducible, low-impact proofs of concept that show real business impact.  
- Document as I go: timestamped notes, commands used, and artifacts saved.

_Practical lab work_  
Pick a small insecure scenario and write the attack path in 10 lines. Keep the PoC compact and repeatable.

---

## Communication, reporting & impact

- Write two summaries per finding: an executive line (what happened, why it matters) and a tech appendix (steps to reproduce).  
- Prioritize fixes by real impact, not by CVSS number. Always ask "who will fix this" and "how long will it take".  
- Be honest and clear. If something is uncertain, mark it as such. Do not inflate.

_Practical lab work_  
For one finding, draft a one-paragraph executive blurb and a short remediation checklist for the ops team.

---

## Legal, safety & professional discipline

- Always confirm scope and contact points first. If in doubt, pause and escalate.  
- Keep proof minimal and reversible when possible. Never intentionally cause production outages.  
- Treat client data with the same care I expect when I'm on the defensive side.

_Practical lab work_  
Draft a minimal Rules of Engagement snippet I can drop into small engagements with emergency contact and kill-switch guidance. 
---

## Continuous learning plan (how I actually improve)

- Weekly: one focused lab exercise mapped to a single fundamental. Short note in Echo.  
- Monthly: one deeper project (exploit chaining, new protocol, or a CTF focused on a weakness). Document in Snare.  
- Quarterly: review past lab notes, retest old PoCs, and convert learnings into reusable templates.

---

## Gaps I want to track (today)

- Windows internals: need a few concrete privilege escalation patterns documented.  
- Cloud identity: need labs that expose IAM misconfigs and privilege chaining.  
- Detection validation: practice noisy vs stealthy activity and see what gets logged in a simple SIEM.

---

